{
    "docs": [
        {
            "location": "/", 
            "text": "DataFrames Documentation Outline\n\n\n\n\nPackage Manual\n\n\n{contents}\nPages = [\nman/getting_started.md\n, \nman/io.md\n, \nman/joins.md\n, \nman/split_apply_combine.md\n, \nman/reshaping_and_pivoting.md\n, \nman/sorting.md\n, \nman/formulas.md\n, \nman/pooling.md\n]\nDepth = 2\n\n\n\n\n\n\n\nAPI\n\n\n{contents}\nPages = [\nlib/maintypes.md\n, \nlib/manipulation.md\n, \nlib/utilities.md\n]\nDepth = 2\n\n\n\n\n\n\n\nDocumentation Index\n\n\n{index}\nPages = [\nlib/maintypes.md\n, \nlib/manipulation.md\n, \nlib/utilities.md\n, \nman/io.md\n]", 
            "title": "Introduction"
        }, 
        {
            "location": "/#dataframes-documentation-outline", 
            "text": "", 
            "title": "DataFrames Documentation Outline"
        }, 
        {
            "location": "/#package-manual", 
            "text": "{contents}\nPages = [ man/getting_started.md ,  man/io.md ,  man/joins.md ,  man/split_apply_combine.md ,  man/reshaping_and_pivoting.md ,  man/sorting.md ,  man/formulas.md ,  man/pooling.md ]\nDepth = 2", 
            "title": "Package Manual"
        }, 
        {
            "location": "/#api", 
            "text": "{contents}\nPages = [ lib/maintypes.md ,  lib/manipulation.md ,  lib/utilities.md ]\nDepth = 2", 
            "title": "API"
        }, 
        {
            "location": "/#documentation-index", 
            "text": "{index}\nPages = [ lib/maintypes.md ,  lib/manipulation.md ,  lib/utilities.md ,  man/io.md ]", 
            "title": "Documentation Index"
        }, 
        {
            "location": "/man/getting_started/", 
            "text": "Getting Started\n\n\n\n\nInstallation\n\n\nThe DataFrames package is available through the Julia package system. Throughout the rest of this tutorial, we will assume that you have installed the DataFrames package and have already typed \nusing DataArrays, DataFrames\n to bring all of the relevant variables into your current namespace. In addition, we will make use of the \nRDatasets\n package, which provides access to hundreds of classical data sets.\n\n\n\n\nThe \nNA\n Value\n\n\nTo get started, let's examine the \nNA\n value. Type the following into the REPL:\n\n\nNA\n\n\n\n\n\n\nOne of the essential properties of \nNA\n is that it poisons other items. To see this, try to add something like \n1\n to \nNA\n:\n\n\n1\n \n+\n \nNA\n\n\n\n\n\n\n\n\nThe \nDataArray\n Type\n\n\nNow that we see that \nNA\n is working, let's insert one into a \nDataArray\n. We'll create one now using the \n@data\n macro:\n\n\ndv\n \n=\n \n@\ndata\n([\nNA\n,\n \n3\n,\n \n2\n,\n \n5\n,\n \n4\n])\n\n\n\n\n\n\nTo see how \nNA\n poisons even complex calculations, let's try to take the mean of the five numbers stored in \ndv\n:\n\n\nmean\n(\ndv\n)\n\n\n\n\n\n\nIn many cases we're willing to just ignore \nNA\n values and remove them from our vector. We can do that using the \ndropna\n function:\n\n\ndropna\n(\ndv\n)\n\n\nmean\n(\ndropna\n(\ndv\n))\n\n\n\n\n\n\nInstead of removing \nNA\n values, you can try to conver the \nDataArray\n into a normal Julia \nArray\n using \nconvert\n:\n\n\nconvert\n(\nArray\n,\n \ndv\n)\n\n\n\n\n\n\nThis fails in the presence of \nNA\n values, but will succeed if there are no \nNA\n values:\n\n\ndv\n[\n1\n]\n \n=\n \n3\n\n\nconvert\n(\nArray\n,\n \ndv\n)\n\n\n\n\n\n\nIn addition to removing \nNA\n values and hoping they won't occur, you can also replace any \nNA\n values using the \nconvert\n function, which takes a replacement value as an argument:\n\n\ndv\n \n=\n \n@\ndata\n([\nNA\n,\n \n3\n,\n \n2\n,\n \n5\n,\n \n4\n])\n\n\nmean\n(\nconvert\n(\nArray\n,\n \ndv\n,\n \n11\n))\n\n\n\n\n\n\nWhich strategy for dealing with \nNA\n values is most appropriate will typically depend on the specific details of your data analysis pathway.\n\n\nAlthough the examples above employed only 1D \nDataArray\n objects, the \nDataArray\n type defines a completely generic N-dimensional array type. Operations on generic \nDataArray\n objects work in higher dimensions in the same way that they work on Julia's Base \nArray\n type:\n\n\ndm\n \n=\n \n@\ndata\n([\nNA\n \n0.0\n;\n \n0.0\n \n1.0\n])\n\n\ndm\n \n*\n \ndm\n\n\n\n\n\n\n\n\nThe \nDataFrame\n Type\n\n\nThe \nDataFrame\n type can be used to represent data tables, each column of which is a \nDataArray\n. You can specify the columns using keyword arguments:\n\n\ndf\n \n=\n \nDataFrame\n(\nA\n \n=\n \n1\n:\n4\n,\n \nB\n \n=\n \n[\nM\n,\n \nF\n,\n \nF\n,\n \nM\n])\n\n\n\n\n\n\nIt is also possible to construct a \nDataFrame\n in stages:\n\n\ndf\n \n=\n \nDataFrame\n()\n\n\ndf\n[:\nA\n]\n \n=\n \n1\n:\n8\n\n\ndf\n[:\nB\n]\n \n=\n \n[\nM\n,\n \nF\n,\n \nF\n,\n \nM\n,\n \nF\n,\n \nM\n,\n \nM\n,\n \nF\n]\n\n\ndf\n\n\n\n\n\n\nThe \nDataFrame\n we build in this way has 8 rows and 2 columns. You can check this using \nsize\n function:\n\n\nnrows\n \n=\n \nsize\n(\ndf\n,\n \n1\n)\n\n\nncols\n \n=\n \nsize\n(\ndf\n,\n \n2\n)\n\n\n\n\n\n\nWe can also look at small subsets of the data in a couple of different ways:\n\n\nhead\n(\ndf\n)\n\n\ntail\n(\ndf\n)\n\n\n\ndf\n[\n1\n:\n3\n,\n \n:]\n\n\n\n\n\n\nHaving seen what some of the rows look like, we can try to summarize the entire data set using \ndescribe\n:\n\n\ndescribe\n(\ndf\n)\n\n\n\n\n\n\nTo focus our search, we start looking at just the means and medians of specific columns. In the example below, we use numeric indexing to access the columns of the \nDataFrame\n:\n\n\nmean\n(\ndf\n[\n1\n])\n\n\nmedian\n(\ndf\n[\n1\n])\n\n\n\n\n\n\nWe could also have used column names to access individual columns:\n\n\nmean\n(\ndf\n[:\nA\n])\n\n\nmedian\n(\ndf\n[:\nA\n])\n\n\n\n\n\n\nWe can also apply a function to each column of a \nDataFrame\n with the \ncolwise\n function. For example:\n\n\ndf\n \n=\n \nDataFrame\n(\nA\n \n=\n \n1\n:\n4\n,\n \nB\n \n=\n \nrandn\n(\n4\n))\n\n\ncolwise\n(\ncumsum\n,\n \ndf\n)\n\n\n\n\n\n\n\n\nAccessing Classic Data Sets\n\n\nTo see more of the functionality for working with \nDataFrame\n objects, we need a more complex data set to work with. We'll use the \nRDatasets\n package, which provides access to many of the classical data sets that are available in R.\n\n\nFor example, we can access Fisher's iris data set using the following functions:\n\n\nusing\n \nRDatasets\n\n\niris\n \n=\n \ndataset\n(\ndatasets\n,\n \niris\n)\n\n\nhead\n(\niris\n)\n\n\n\n\n\n\nIn the next section, we'll discuss generic I/O strategy for reading and writing \nDataFrame\n objects that you can use to import and export your own data files.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/man/getting_started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/man/getting_started/#installation", 
            "text": "The DataFrames package is available through the Julia package system. Throughout the rest of this tutorial, we will assume that you have installed the DataFrames package and have already typed  using DataArrays, DataFrames  to bring all of the relevant variables into your current namespace. In addition, we will make use of the  RDatasets  package, which provides access to hundreds of classical data sets.", 
            "title": "Installation"
        }, 
        {
            "location": "/man/getting_started/#the-na-value", 
            "text": "To get started, let's examine the  NA  value. Type the following into the REPL:  NA   One of the essential properties of  NA  is that it poisons other items. To see this, try to add something like  1  to  NA :  1   +   NA", 
            "title": "The NA Value"
        }, 
        {
            "location": "/man/getting_started/#the-dataarray-type", 
            "text": "Now that we see that  NA  is working, let's insert one into a  DataArray . We'll create one now using the  @data  macro:  dv   =   @ data ([ NA ,   3 ,   2 ,   5 ,   4 ])   To see how  NA  poisons even complex calculations, let's try to take the mean of the five numbers stored in  dv :  mean ( dv )   In many cases we're willing to just ignore  NA  values and remove them from our vector. We can do that using the  dropna  function:  dropna ( dv )  mean ( dropna ( dv ))   Instead of removing  NA  values, you can try to conver the  DataArray  into a normal Julia  Array  using  convert :  convert ( Array ,   dv )   This fails in the presence of  NA  values, but will succeed if there are no  NA  values:  dv [ 1 ]   =   3  convert ( Array ,   dv )   In addition to removing  NA  values and hoping they won't occur, you can also replace any  NA  values using the  convert  function, which takes a replacement value as an argument:  dv   =   @ data ([ NA ,   3 ,   2 ,   5 ,   4 ])  mean ( convert ( Array ,   dv ,   11 ))   Which strategy for dealing with  NA  values is most appropriate will typically depend on the specific details of your data analysis pathway.  Although the examples above employed only 1D  DataArray  objects, the  DataArray  type defines a completely generic N-dimensional array type. Operations on generic  DataArray  objects work in higher dimensions in the same way that they work on Julia's Base  Array  type:  dm   =   @ data ([ NA   0.0 ;   0.0   1.0 ])  dm   *   dm", 
            "title": "The DataArray Type"
        }, 
        {
            "location": "/man/getting_started/#the-dataframe-type", 
            "text": "The  DataFrame  type can be used to represent data tables, each column of which is a  DataArray . You can specify the columns using keyword arguments:  df   =   DataFrame ( A   =   1 : 4 ,   B   =   [ M ,   F ,   F ,   M ])   It is also possible to construct a  DataFrame  in stages:  df   =   DataFrame ()  df [: A ]   =   1 : 8  df [: B ]   =   [ M ,   F ,   F ,   M ,   F ,   M ,   M ,   F ]  df   The  DataFrame  we build in this way has 8 rows and 2 columns. You can check this using  size  function:  nrows   =   size ( df ,   1 )  ncols   =   size ( df ,   2 )   We can also look at small subsets of the data in a couple of different ways:  head ( df )  tail ( df )  df [ 1 : 3 ,   :]   Having seen what some of the rows look like, we can try to summarize the entire data set using  describe :  describe ( df )   To focus our search, we start looking at just the means and medians of specific columns. In the example below, we use numeric indexing to access the columns of the  DataFrame :  mean ( df [ 1 ])  median ( df [ 1 ])   We could also have used column names to access individual columns:  mean ( df [: A ])  median ( df [: A ])   We can also apply a function to each column of a  DataFrame  with the  colwise  function. For example:  df   =   DataFrame ( A   =   1 : 4 ,   B   =   randn ( 4 ))  colwise ( cumsum ,   df )", 
            "title": "The DataFrame Type"
        }, 
        {
            "location": "/man/getting_started/#accessing-classic-data-sets", 
            "text": "To see more of the functionality for working with  DataFrame  objects, we need a more complex data set to work with. We'll use the  RDatasets  package, which provides access to many of the classical data sets that are available in R.  For example, we can access Fisher's iris data set using the following functions:  using   RDatasets  iris   =   dataset ( datasets ,   iris )  head ( iris )   In the next section, we'll discuss generic I/O strategy for reading and writing  DataFrame  objects that you can use to import and export your own data files.", 
            "title": "Accessing Classic Data Sets"
        }, 
        {
            "location": "/man/io/", 
            "text": "Importing and Exporting (I/O)\n\n\n\n\nImporting data from tabular data files\n\n\nTo read data from a CSV-like file, use the \nreadtable\n function:\n\n\n{docs}\nreadtable\n\n\n\n\n\nreadtable\n requires that you specify the path of the file that you would like to read as a \nString\n. To read data from a non-file source, you may also supply an \nIO\n object. It supports many additional keyword arguments: these are documented in the section on advanced I/O operations.\n\n\n\n\nExporting data to a tabular data file\n\n\nTo write data to a CSV file, use the \nwritetable\n function:\n\n\n{docs}\nwritetable\n\n\n\n\n\n\n\nSupplying \nDataFrame\ns inline with non-standard string literals\n\n\nYou can also provide CSV-like tabular data in a non-standard string literal to construct a new \nDataFrame\n, as in the following:\n\n\ndf\n \n=\n \ncsv\n\n\n    name,  age, squidPerWeek\n\n\n    Alice,  36,         3.14\n\n\n    Bob,    24,         0\n\n\n    Carol,  58,         2.71\n\n\n    Eve,    49,         7.77\n\n\n    \n\n\n\n\n\n\nThe \ncsv\n string literal prefix indicates that the data are supplied in standard comma-separated value format. Common alternative formats are also available as string literals. For semicolon-separated values, with comma as a decimal, use \ncsv2\n:\n\n\ndf\n \n=\n \ncsv2\n\n\n    name;  age; squidPerWeek\n\n\n    Alice;  36;         3,14\n\n\n    Bob;    24;         0\n\n\n    Carol;  58;         2,71\n\n\n    Eve;    49;         7,77\n\n\n    \n\n\n\n\n\n\nFor whitespace-separated values, use \nwsv\n:\n\n\ndf\n \n=\n \nwsv\n\n\n    name  age squidPerWeek\n\n\n    Alice  36         3.14\n\n\n    Bob    24         0\n\n\n    Carol  58         2.71\n\n\n    Eve    49         7.77\n\n\n    \n\n\n\n\n\n\nAnd for tab-separated values, use \ntsv\n:\n\n\ndf\n \n=\n \ntsv\n\n\n    name    age squidPerWeek\n\n\n    Alice   36  3.14\n\n\n    Bob 24  0\n\n\n    Carol   58  2.71\n\n\n    Eve 49  7.77", 
            "title": "IO"
        }, 
        {
            "location": "/man/io/#importing-and-exporting-io", 
            "text": "", 
            "title": "Importing and Exporting (I/O)"
        }, 
        {
            "location": "/man/io/#importing-data-from-tabular-data-files", 
            "text": "To read data from a CSV-like file, use the  readtable  function:  {docs}\nreadtable  readtable  requires that you specify the path of the file that you would like to read as a  String . To read data from a non-file source, you may also supply an  IO  object. It supports many additional keyword arguments: these are documented in the section on advanced I/O operations.", 
            "title": "Importing data from tabular data files"
        }, 
        {
            "location": "/man/io/#exporting-data-to-a-tabular-data-file", 
            "text": "To write data to a CSV file, use the  writetable  function:  {docs}\nwritetable", 
            "title": "Exporting data to a tabular data file"
        }, 
        {
            "location": "/man/io/#supplying-dataframes-inline-with-non-standard-string-literals", 
            "text": "You can also provide CSV-like tabular data in a non-standard string literal to construct a new  DataFrame , as in the following:  df   =   csv      name,  age, squidPerWeek      Alice,  36,         3.14      Bob,    24,         0      Carol,  58,         2.71      Eve,    49,         7.77         The  csv  string literal prefix indicates that the data are supplied in standard comma-separated value format. Common alternative formats are also available as string literals. For semicolon-separated values, with comma as a decimal, use  csv2 :  df   =   csv2      name;  age; squidPerWeek      Alice;  36;         3,14      Bob;    24;         0      Carol;  58;         2,71      Eve;    49;         7,77         For whitespace-separated values, use  wsv :  df   =   wsv      name  age squidPerWeek      Alice  36         3.14      Bob    24         0      Carol  58         2.71      Eve    49         7.77         And for tab-separated values, use  tsv :  df   =   tsv      name    age squidPerWeek      Alice   36  3.14      Bob 24  0      Carol   58  2.71      Eve 49  7.77", 
            "title": "Supplying DataFrames inline with non-standard string literals"
        }, 
        {
            "location": "/man/joins/", 
            "text": "Database-Style Joins\n\n\nWe often need to combine two or more data sets together to provide a complete picture of the topic we are studying. For example, suppose that we have the following two data sets:\n\n\nnames\n \n=\n \nDataFrame\n(\nID\n \n=\n \n[\n1\n,\n \n2\n],\n \nName\n \n=\n \n[\nJohn Doe\n,\n \nJane Doe\n])\n\n\njobs\n \n=\n \nDataFrame\n(\nID\n \n=\n \n[\n1\n,\n \n2\n],\n \nJob\n \n=\n \n[\nLawyer\n,\n \nDoctor\n])\n\n\n\n\n\n\nWe might want to work with a larger data set that contains both the names and jobs for each ID. We can do this using the \njoin\n function:\n\n\nfull\n \n=\n \njoin\n(\nnames\n,\n \njobs\n,\n \non\n \n=\n \n:\nID\n)\n\n\n\n\n\n\nOutput:\n\n\n\n\n\n\n\n\nRow\n\n\nID\n\n\nName\n\n\nJob\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\"John Doe\"\n\n\n\"Lawyer\"\n\n\n\n\n\n\n2\n\n\n1\n\n\n\"Jane Doe\"\n\n\n\"Doctor\"\n\n\n\n\n\n\n\n\nIn relational database theory, this operation is generally referred to as a join. The columns used to determine which rows should be combined during a join are called keys.\n\n\nThere are seven kinds of joins supported by the DataFrames package:\n\n\n\n\nInner: The output contains rows for values of the key that exist in both the first (left) and second (right) arguments to \njoin\n.\n\n\nLeft: The output contains rows for values of the key that exist in the first (left) argument to \njoin\n, whether or not that value exists in the second (right) argument.\n\n\nRight: The output contains rows for values of the key that exist in the second (right) argument to \njoin\n, whether or not that value exists in the first (left) argument.\n\n\nOuter: The output contains rows for values of the key that exist in the first (left) or second (right) argument to \njoin\n.\n\n\nSemi: Like an inner join, but output is restricted to columns from the first (left) argument to \njoin\n.\n\n\nAnti: The output contains rows for values of the key that exist in the first (left) but not the second (right) argument to \njoin\n. As with semi joins, output is restricted to columns from the first (left) argument.\n\n\nCross: The output is the cartesian product of rows from the first (left) and second (right) arguments to \njoin\n.\n\n\n\n\nYou can control the kind of join that \njoin\n performs using the \nkind\n keyword argument:\n\n\na\n \n=\n \nDataFrame\n(\nID\n \n=\n \n[\n1\n,\n \n2\n],\n \nName\n \n=\n \n[\nA\n,\n \nB\n])\n\n\nb\n \n=\n \nDataFrame\n(\nID\n \n=\n \n[\n1\n,\n \n3\n],\n \nJob\n \n=\n \n[\nDoctor\n,\n \nLawyer\n])\n\n\njoin\n(\na\n,\n \nb\n,\n \non\n \n=\n \n:\nID\n,\n \nkind\n \n=\n \n:\ninner\n)\n\n\njoin\n(\na\n,\n \nb\n,\n \non\n \n=\n \n:\nID\n,\n \nkind\n \n=\n \n:\nleft\n)\n\n\njoin\n(\na\n,\n \nb\n,\n \non\n \n=\n \n:\nID\n,\n \nkind\n \n=\n \n:\nright\n)\n\n\njoin\n(\na\n,\n \nb\n,\n \non\n \n=\n \n:\nID\n,\n \nkind\n \n=\n \n:\nouter\n)\n\n\njoin\n(\na\n,\n \nb\n,\n \non\n \n=\n \n:\nID\n,\n \nkind\n \n=\n \n:\nsemi\n)\n\n\njoin\n(\na\n,\n \nb\n,\n \non\n \n=\n \n:\nID\n,\n \nkind\n \n=\n \n:\nanti\n)\n\n\n\n\n\n\nCross joins are the only kind of join that does not use a key:\n\n\njoin\n(\na\n,\n \nb\n,\n \nkind\n \n=\n \n:\ncross\n)", 
            "title": "Joins"
        }, 
        {
            "location": "/man/joins/#database-style-joins", 
            "text": "We often need to combine two or more data sets together to provide a complete picture of the topic we are studying. For example, suppose that we have the following two data sets:  names   =   DataFrame ( ID   =   [ 1 ,   2 ],   Name   =   [ John Doe ,   Jane Doe ])  jobs   =   DataFrame ( ID   =   [ 1 ,   2 ],   Job   =   [ Lawyer ,   Doctor ])   We might want to work with a larger data set that contains both the names and jobs for each ID. We can do this using the  join  function:  full   =   join ( names ,   jobs ,   on   =   : ID )   Output:     Row  ID  Name  Job      1  1  \"John Doe\"  \"Lawyer\"    2  1  \"Jane Doe\"  \"Doctor\"     In relational database theory, this operation is generally referred to as a join. The columns used to determine which rows should be combined during a join are called keys.  There are seven kinds of joins supported by the DataFrames package:   Inner: The output contains rows for values of the key that exist in both the first (left) and second (right) arguments to  join .  Left: The output contains rows for values of the key that exist in the first (left) argument to  join , whether or not that value exists in the second (right) argument.  Right: The output contains rows for values of the key that exist in the second (right) argument to  join , whether or not that value exists in the first (left) argument.  Outer: The output contains rows for values of the key that exist in the first (left) or second (right) argument to  join .  Semi: Like an inner join, but output is restricted to columns from the first (left) argument to  join .  Anti: The output contains rows for values of the key that exist in the first (left) but not the second (right) argument to  join . As with semi joins, output is restricted to columns from the first (left) argument.  Cross: The output is the cartesian product of rows from the first (left) and second (right) arguments to  join .   You can control the kind of join that  join  performs using the  kind  keyword argument:  a   =   DataFrame ( ID   =   [ 1 ,   2 ],   Name   =   [ A ,   B ])  b   =   DataFrame ( ID   =   [ 1 ,   3 ],   Job   =   [ Doctor ,   Lawyer ])  join ( a ,   b ,   on   =   : ID ,   kind   =   : inner )  join ( a ,   b ,   on   =   : ID ,   kind   =   : left )  join ( a ,   b ,   on   =   : ID ,   kind   =   : right )  join ( a ,   b ,   on   =   : ID ,   kind   =   : outer )  join ( a ,   b ,   on   =   : ID ,   kind   =   : semi )  join ( a ,   b ,   on   =   : ID ,   kind   =   : anti )   Cross joins are the only kind of join that does not use a key:  join ( a ,   b ,   kind   =   : cross )", 
            "title": "Database-Style Joins"
        }, 
        {
            "location": "/man/split_apply_combine/", 
            "text": "The Split-Apply-Combine Strategy\n\n\nMany data analysis tasks involve splitting a data set into groups, applying some functions to each of the groups and then combining the results. A standardized framework for handling this sort of computation is described in the paper, The Split-Apply-Combine Strategy for Data Analysis \\\nhttp://www.jstatsoft.org/v40/i01\n>, written by Hadley Wickham.\n\n\nThe DataFrames package supports the Split-Apply-Combine strategy through the \nby\n function, which takes in three arguments: (1) a DataFrame, (2) a column to split the DataFrame on, and (3) a function or expression to apply to each subset of the DataFrame.\n\n\nWe show several examples of the \nby\n function applied to the \niris\n dataset below:\n\n\nusing\n \nDataFrames\n,\n \nRDatasets\n\n\n\niris\n \n=\n \ndataset\n(\ndatasets\n,\n \niris\n)\n\n\n\nby\n(\niris\n,\n \n:\nSpecies\n,\n \nsize\n)\n\n\nby\n(\niris\n,\n \n:\nSpecies\n,\n \ndf\n \n-\n \nmean\n(\ndf\n[:\nPetalLength\n]))\n\n\nby\n(\niris\n,\n \n:\nSpecies\n,\n \ndf\n \n-\n \nDataFrame\n(\nN\n \n=\n \nsize\n(\ndf\n,\n \n1\n)))\n\n\n\n\n\n\nThe \nby\n function also support the \ndo\n block form:\n\n\nby\n(\niris\n,\n \n:\nSpecies\n)\n \ndo\n \ndf\n\n   \nDataFrame\n(\nm\n \n=\n \nmean\n(\ndf\n[:\nPetalLength\n]),\n \ns\u00b2\n \n=\n \nvar\n(\ndf\n[:\nPetalLength\n]))\n\n\nend\n\n\n\n\n\n\nA second approach to the Split-Apply-Combine strategy is implemented in the \naggregate\n function, which also takes three arguments: (1) a DataFrame, (2) a column (or columns) to split the DataFrame on, and a (3) function (or several functions) that are used to compute a summary of each subset of the DataFrame. Each function is applied to each column, that was not used to split the DataFrame, creating new columns of the form \n$name_$function\n e.g. \nSepalLength_mean\n. Anonymous functions and expressions that do not have a name will be called \n\u03bb1\n.\n\n\nWe show several examples of the \naggregate\n function applied to the \niris\n dataset below:\n\n\naggregate\n(\niris\n,\n \n:\nSpecies\n,\n \nsum\n)\n\n\naggregate\n(\niris\n,\n \n:\nSpecies\n,\n \n[\nsum\n,\n \nmean\n])\n\n\n\n\n\n\nIf you only want to split the data set into subsets, use the \ngroupby\n function:\n\n\nfor\n \nsubdf\n \nin\n \ngroupby\n(\niris\n,\n \n:\nSpecies\n)\n\n    \nprintln\n(\nsize\n(\nsubdf\n,\n \n1\n))\n\n\nend", 
            "title": "Split-apply-combine"
        }, 
        {
            "location": "/man/split_apply_combine/#the-split-apply-combine-strategy", 
            "text": "Many data analysis tasks involve splitting a data set into groups, applying some functions to each of the groups and then combining the results. A standardized framework for handling this sort of computation is described in the paper, The Split-Apply-Combine Strategy for Data Analysis \\ http://www.jstatsoft.org/v40/i01 >, written by Hadley Wickham.  The DataFrames package supports the Split-Apply-Combine strategy through the  by  function, which takes in three arguments: (1) a DataFrame, (2) a column to split the DataFrame on, and (3) a function or expression to apply to each subset of the DataFrame.  We show several examples of the  by  function applied to the  iris  dataset below:  using   DataFrames ,   RDatasets  iris   =   dataset ( datasets ,   iris )  by ( iris ,   : Species ,   size )  by ( iris ,   : Species ,   df   -   mean ( df [: PetalLength ]))  by ( iris ,   : Species ,   df   -   DataFrame ( N   =   size ( df ,   1 )))   The  by  function also support the  do  block form:  by ( iris ,   : Species )   do   df \n    DataFrame ( m   =   mean ( df [: PetalLength ]),   s\u00b2   =   var ( df [: PetalLength ]))  end   A second approach to the Split-Apply-Combine strategy is implemented in the  aggregate  function, which also takes three arguments: (1) a DataFrame, (2) a column (or columns) to split the DataFrame on, and a (3) function (or several functions) that are used to compute a summary of each subset of the DataFrame. Each function is applied to each column, that was not used to split the DataFrame, creating new columns of the form  $name_$function  e.g.  SepalLength_mean . Anonymous functions and expressions that do not have a name will be called  \u03bb1 .  We show several examples of the  aggregate  function applied to the  iris  dataset below:  aggregate ( iris ,   : Species ,   sum )  aggregate ( iris ,   : Species ,   [ sum ,   mean ])   If you only want to split the data set into subsets, use the  groupby  function:  for   subdf   in   groupby ( iris ,   : Species ) \n     println ( size ( subdf ,   1 ))  end", 
            "title": "The Split-Apply-Combine Strategy"
        }, 
        {
            "location": "/man/reshaping_and_pivoting/", 
            "text": "Reshaping and Pivoting Data\n\n\nReshape data from wide to long format using the \nstack\n function:\n\n\nusing\n \nDataFrames\n,\n \nRDatasets\n\n\niris\n \n=\n \ndataset\n(\ndatasets\n,\n \niris\n)\n\n\niris\n[:\nid\n]\n \n=\n \n1\n:\nsize\n(\niris\n,\n \n1\n)\n  \n# this makes it easier to unstack\n\n\nd\n \n=\n \nstack\n(\niris\n,\n \n[\n1\n:\n4\n])\n\n\n\n\n\n\nThe second optional argument to \nstack\n indicates the columns to be stacked. These are normally referred to as the measured variables. Column names can also be given:\n\n\nd\n \n=\n \nstack\n(\niris\n,\n \n[:\nSepalLength\n,\n \n:\nSepalWidth\n,\n \n:\nPetalLength\n,\n \n:\nPetalWidth\n])\n\n\n\n\n\n\nNote that all columns can be of different types. Type promotion follows the rules of \nvcat\n.\n\n\nThe stacked DataFrame that results includes all of the columns not specified to be stacked. These are repeated for each stacked column. These are normally refered to as identifier (id) columns. In addition to the id columns, two additional columns labeled \n:variable\n and \n:values\n contain the column identifier and the stacked columns.\n\n\nA third optional argument to \nstack\n represents the id columns that are repeated. This makes it easier to specify which variables you want included in the long format:\n\n\nd\n \n=\n \nstack\n(\niris\n,\n \n[:\nSepalLength\n,\n \n:\nSepalWidth\n],\n \n:\nSpecies\n)\n\n\n\n\n\n\nmelt\n is an alternative function to reshape from wide to long format. It is based on \nstack\n, but it prefers specification of the id columns as:\n\n\nd\n \n=\n \nmelt\n(\niris\n,\n \n:\nSpecies\n)\n\n\n\n\n\n\nAll other columns are assumed to be measured variables (they are stacked).\n\n\nYou can also stack an entire DataFrame. The default stacks all floating-point columns:\n\n\nd\n \n=\n \nstack\n(\niris\n)\n\n\n\n\n\n\nunstack\n converts from a long format to a wide format. The default is requires specifying which columns are an id variable, column variable names, and column values:\n\n\nlongdf\n \n=\n \nmelt\n(\niris\n,\n \n[:\nSpecies\n,\n \n:\nid\n])\n\n\nwidedf\n \n=\n \nunstack\n(\nlongdf\n,\n \n:\nid\n,\n \n:\nvariable\n,\n \n:\nvalue\n)\n\n\n\n\n\n\nIf the remaining columns are unique, you can skip the id variable and use:\n\n\nwidedf\n \n=\n \nunstack\n(\nlongdf\n,\n \n:\nvariable\n,\n \n:\nvalue\n)\n\n\n\n\n\n\nstackdf\n and \nmeltdf\n are two additional functions that work like \nstack\n and \nmelt\n, but they provide a view into the original wide DataFrame. Here is an example:\n\n\nd\n \n=\n \nstackdf\n(\niris\n)\n\n\n\n\n\n\nThis saves memory. To create the view, several AbstractVectors are defined:\n\n\n:variable\n column \u2013 \nEachRepeatedVector\n   This repeats the variables N times where N is the number of rows of the original AbstractDataFrame.\n\n\n:value\n column \u2013 \nStackedVector\n   This is provides a view of the original columns stacked together.\n\n\nId columns \u2013 \nRepeatedVector\n   This repeats the original columns N times where N is the number of columns stacked.\n\n\nFor more details on the storage representation, see:\n\n\ndump\n(\nstackdf\n(\niris\n))\n\n\n\n\n\n\nNone of these reshaping functions perform any aggregation. To do aggregation, use the split-apply-combine functions in combination with reshaping. Here is an example:\n\n\nd\n \n=\n \nstack\n(\niris\n)\n\n\nx\n \n=\n \nby\n(\nd\n,\n \n[:\nvariable\n,\n \n:\nSpecies\n],\n \ndf\n \n-\n \nDataFrame\n(\nvsum\n \n=\n \nmean\n(\ndf\n[:\nvalue\n])))\n\n\nunstack\n(\nx\n,\n \n:\nSpecies\n,\n \n:\nvsum\n)", 
            "title": "Reshaping"
        }, 
        {
            "location": "/man/reshaping_and_pivoting/#reshaping-and-pivoting-data", 
            "text": "Reshape data from wide to long format using the  stack  function:  using   DataFrames ,   RDatasets  iris   =   dataset ( datasets ,   iris )  iris [: id ]   =   1 : size ( iris ,   1 )    # this makes it easier to unstack  d   =   stack ( iris ,   [ 1 : 4 ])   The second optional argument to  stack  indicates the columns to be stacked. These are normally referred to as the measured variables. Column names can also be given:  d   =   stack ( iris ,   [: SepalLength ,   : SepalWidth ,   : PetalLength ,   : PetalWidth ])   Note that all columns can be of different types. Type promotion follows the rules of  vcat .  The stacked DataFrame that results includes all of the columns not specified to be stacked. These are repeated for each stacked column. These are normally refered to as identifier (id) columns. In addition to the id columns, two additional columns labeled  :variable  and  :values  contain the column identifier and the stacked columns.  A third optional argument to  stack  represents the id columns that are repeated. This makes it easier to specify which variables you want included in the long format:  d   =   stack ( iris ,   [: SepalLength ,   : SepalWidth ],   : Species )   melt  is an alternative function to reshape from wide to long format. It is based on  stack , but it prefers specification of the id columns as:  d   =   melt ( iris ,   : Species )   All other columns are assumed to be measured variables (they are stacked).  You can also stack an entire DataFrame. The default stacks all floating-point columns:  d   =   stack ( iris )   unstack  converts from a long format to a wide format. The default is requires specifying which columns are an id variable, column variable names, and column values:  longdf   =   melt ( iris ,   [: Species ,   : id ])  widedf   =   unstack ( longdf ,   : id ,   : variable ,   : value )   If the remaining columns are unique, you can skip the id variable and use:  widedf   =   unstack ( longdf ,   : variable ,   : value )   stackdf  and  meltdf  are two additional functions that work like  stack  and  melt , but they provide a view into the original wide DataFrame. Here is an example:  d   =   stackdf ( iris )   This saves memory. To create the view, several AbstractVectors are defined:  :variable  column \u2013  EachRepeatedVector    This repeats the variables N times where N is the number of rows of the original AbstractDataFrame.  :value  column \u2013  StackedVector    This is provides a view of the original columns stacked together.  Id columns \u2013  RepeatedVector    This repeats the original columns N times where N is the number of columns stacked.  For more details on the storage representation, see:  dump ( stackdf ( iris ))   None of these reshaping functions perform any aggregation. To do aggregation, use the split-apply-combine functions in combination with reshaping. Here is an example:  d   =   stack ( iris )  x   =   by ( d ,   [: variable ,   : Species ],   df   -   DataFrame ( vsum   =   mean ( df [: value ])))  unstack ( x ,   : Species ,   : vsum )", 
            "title": "Reshaping and Pivoting Data"
        }, 
        {
            "location": "/man/sorting/", 
            "text": "Sorting\n\n\nSorting is a fundamental component of data analysis. Basic sorting is trivial: just calling \nsort!\n will sort all columns, in place:\n\n\nusing\n \nDataFrames\n,\n \nRDatasets\n\n\n\niris\n \n=\n \ndataset\n(\ndatasets\n,\n \niris\n)\n\n\nsort!\n(\niris\n)\n\n\n\n\n\n\nIn Sorting DataFrames, you may want to sort different columns with different options. Here are some examples showing most of the possible options:\n\n\nsort!\n(\niris\n,\n \nrev\n \n=\n \ntrue\n)\n\n\nsort!\n(\niris\n,\n \ncols\n \n=\n \n[:\nSepalWidth\n,\n \n:\nSepalLength\n])\n\n\n\nsort!\n(\niris\n,\n \ncols\n \n=\n \n[\norder\n(:\nSpecies\n,\n \nby\n \n=\n \nuppercase\n),\n\n                    \norder\n(:\nSepalLength\n,\n \nrev\n \n=\n \ntrue\n)])\n\n\n\n\n\n\nKeywords used above include \ncols\n (to specify columns), \nrev\n (to sort a column or the whole DataFrame in reverse), and \nby\n (to apply a function to a column/DataFrame). Each keyword can either be a single value, or can be a tuple or array, with values corresponding to individual columns.\n\n\nAs an alternative to using array or tuple values, \norder\n to specify an ordering for a particular column within a set of columns\n\n\nThe following two examples show two ways to sort the \niris\n dataset with the same result: \nSpecies\n will be ordered in reverse lexicographic order, and within species, rows will be sorted by increasing sepal length and width:\n\n\nsort!\n(\niris\n,\n \ncols\n \n=\n \n(:\nSpecies\n,\n \n:\nSepalLength\n,\n \n:\nSepalWidth\n),\n\n      \nrev\n \n=\n \n(\ntrue\n,\n \nfalse\n,\n \nfalse\n))\n\n\n\nsort!\n(\niris\n,\n \ncols\n \n=\n \n(\norder\n(:\nSpecies\n,\n \nrev\n \n=\n \ntrue\n),\n \n:\nSepalLength\n,\n \n:\nSepalWidth\n))", 
            "title": "Sorting"
        }, 
        {
            "location": "/man/sorting/#sorting", 
            "text": "Sorting is a fundamental component of data analysis. Basic sorting is trivial: just calling  sort!  will sort all columns, in place:  using   DataFrames ,   RDatasets  iris   =   dataset ( datasets ,   iris )  sort! ( iris )   In Sorting DataFrames, you may want to sort different columns with different options. Here are some examples showing most of the possible options:  sort! ( iris ,   rev   =   true )  sort! ( iris ,   cols   =   [: SepalWidth ,   : SepalLength ])  sort! ( iris ,   cols   =   [ order (: Species ,   by   =   uppercase ), \n                     order (: SepalLength ,   rev   =   true )])   Keywords used above include  cols  (to specify columns),  rev  (to sort a column or the whole DataFrame in reverse), and  by  (to apply a function to a column/DataFrame). Each keyword can either be a single value, or can be a tuple or array, with values corresponding to individual columns.  As an alternative to using array or tuple values,  order  to specify an ordering for a particular column within a set of columns  The following two examples show two ways to sort the  iris  dataset with the same result:  Species  will be ordered in reverse lexicographic order, and within species, rows will be sorted by increasing sepal length and width:  sort! ( iris ,   cols   =   (: Species ,   : SepalLength ,   : SepalWidth ), \n       rev   =   ( true ,   false ,   false ))  sort! ( iris ,   cols   =   ( order (: Species ,   rev   =   true ),   : SepalLength ,   : SepalWidth ))", 
            "title": "Sorting"
        }, 
        {
            "location": "/man/formulas/", 
            "text": "The Formula, ModelFrame and ModelMatrix Types\n\n\nIn regression analysis, we often want to describe the relationship between a response variable and one or more input variables in terms of main effects and interactions. To facilitate the specification of a regression model in terms of the columns of a \nDataFrame\n, the DataFrames package provides a \nFormula\n type, which is created by the \n~\n binary operator in Julia:\n\n\nfm\n \n=\n \nZ\n \n~\n \nX\n \n+\n \nY\n\n\n\n\n\n\nA \nFormula\n object can be used to transform a \nDataFrame\n into a \nModelFrame\n object:\n\n\ndf\n \n=\n \nDataFrame\n(\nX\n \n=\n \nrandn\n(\n10\n),\n \nY\n \n=\n \nrandn\n(\n10\n),\n \nZ\n \n=\n \nrandn\n(\n10\n))\n\n\nmf\n \n=\n \nModelFrame\n(\nZ\n \n~\n \nX\n \n+\n \nY\n,\n \ndf\n)\n\n\n\n\n\n\nA \nModelFrame\n object is just a simple wrapper around a \nDataFrame\n. For modeling purposes, one generally wants to construct a \nModelMatrix\n, which constructs a \nMatrix{Float64}\n that can be used directly to fit a statistical model:\n\n\nmm\n \n=\n \nModelMatrix\n(\nModelFrame\n(\nZ\n \n~\n \nX\n \n+\n \nY\n,\n \ndf\n))\n\n\n\n\n\n\nNote that \nmm\n contains an additional column consisting entirely of \n1.0\n values. This is used to fit an intercept term in a regression model.\n\n\nIn addition to specifying main effects, it is possible to specify interactions using the \n operator inside a \nFormula\n:\n\n\nmm\n \n=\n \nModelMatrix\n(\nModelFrame\n(\nZ\n \n~\n \nX\n \n+\n \nY\n \n+\n \nX\nY\n,\n \ndf\n))\n\n\n\n\n\n\nIf you would like to specify both main effects and an interaction term at once, use the \n*\n operator inside a \nFormula\n:\n\n\nmm\n \n=\n \nModelMatrix\n(\nModelFrame\n(\nZ\n \n~\n \nX\n*\nY\n,\n \ndf\n))\n\n\n\n\n\n\nYou can control how categorical variables (e.g., \nPooledDataArray\n columns) are converted to \nModelMatrix\n columns by specifying \ncontrasts\n when you construct a \nModelFrame\n:\n\n\nmm\n \n=\n \nModelMatrix\n(\nModelFrame\n(\nZ\n \n~\n \nX\n*\nY\n,\n \ndf\n,\n \ncontrasts\n \n=\n \nDict\n(:\nX\n \n=\n \nHelmertCoding\n())))\n\n\n\n\n\n\nContrasts can also be modified in an existing \nModelFrame\n:\n\n\nmf\n \n=\n \nModelFrame\n(\nZ\n \n~\n \nX\n*\nY\n,\n \ndf\n)\n\n\ncontrasts!\n(\nmf\n,\n \nX\n \n=\n \nHelmertCoding\n())\n\n\n\n\n\n\nThe construction of model matrices makes it easy to formulate complex statistical models. These are used to good effect by the \nGLM Package.", 
            "title": "Formulas"
        }, 
        {
            "location": "/man/formulas/#the-formula-modelframe-and-modelmatrix-types", 
            "text": "In regression analysis, we often want to describe the relationship between a response variable and one or more input variables in terms of main effects and interactions. To facilitate the specification of a regression model in terms of the columns of a  DataFrame , the DataFrames package provides a  Formula  type, which is created by the  ~  binary operator in Julia:  fm   =   Z   ~   X   +   Y   A  Formula  object can be used to transform a  DataFrame  into a  ModelFrame  object:  df   =   DataFrame ( X   =   randn ( 10 ),   Y   =   randn ( 10 ),   Z   =   randn ( 10 ))  mf   =   ModelFrame ( Z   ~   X   +   Y ,   df )   A  ModelFrame  object is just a simple wrapper around a  DataFrame . For modeling purposes, one generally wants to construct a  ModelMatrix , which constructs a  Matrix{Float64}  that can be used directly to fit a statistical model:  mm   =   ModelMatrix ( ModelFrame ( Z   ~   X   +   Y ,   df ))   Note that  mm  contains an additional column consisting entirely of  1.0  values. This is used to fit an intercept term in a regression model.  In addition to specifying main effects, it is possible to specify interactions using the   operator inside a  Formula :  mm   =   ModelMatrix ( ModelFrame ( Z   ~   X   +   Y   +   X Y ,   df ))   If you would like to specify both main effects and an interaction term at once, use the  *  operator inside a  Formula :  mm   =   ModelMatrix ( ModelFrame ( Z   ~   X * Y ,   df ))   You can control how categorical variables (e.g.,  PooledDataArray  columns) are converted to  ModelMatrix  columns by specifying  contrasts  when you construct a  ModelFrame :  mm   =   ModelMatrix ( ModelFrame ( Z   ~   X * Y ,   df ,   contrasts   =   Dict (: X   =   HelmertCoding ())))   Contrasts can also be modified in an existing  ModelFrame :  mf   =   ModelFrame ( Z   ~   X * Y ,   df )  contrasts! ( mf ,   X   =   HelmertCoding ())   The construction of model matrices makes it easy to formulate complex statistical models. These are used to good effect by the  GLM Package.", 
            "title": "The Formula, ModelFrame and ModelMatrix Types"
        }, 
        {
            "location": "/man/pooling/", 
            "text": "Pooling Data (Representing Factors)\n\n\nOften, we have to deal with factors that take on a small number of levels:\n\n\ndv\n \n=\n \n@\ndata\n([\nGroup A\n,\n \nGroup A\n,\n \nGroup A\n,\n\n            \nGroup B\n,\n \nGroup B\n,\n \nGroup B\n])\n\n\n\n\n\n\nThe naive encoding used in a \nDataArray\n represents every entry of this vector as a full string. In contrast, we can represent the data more efficiently by replacing the strings with indices into a small pool of levels. This is what the \nPooledDataArray\n does:\n\n\npdv\n \n=\n \n@\npdata\n([\nGroup A\n,\n \nGroup A\n,\n \nGroup A\n,\n\n              \nGroup B\n,\n \nGroup B\n,\n \nGroup B\n])\n\n\n\n\n\n\nIn addition to representing repeated data efficiently, the \nPooledDataArray\n allows us to determine the levels of the factor at any time using the \nlevels\n function:\n\n\nlevels\n(\npdv\n)\n\n\n\n\n\n\nBy default, a \nPooledDataArray\n is able to represent 2\n32\ndifferents levels. You can use less memory by calling the \ncompact\n function:\n\n\npdv\n \n=\n \ncompact\n(\npdv\n)\n\n\n\n\n\n\nOften, you will have factors encoded inside a DataFrame with \nDataArray\n columns instead of \nPooledDataArray\n columns. You can do conversion of a single column using the \npool\n function:\n\n\npdv\n \n=\n \npool\n(\ndv\n)\n\n\n\n\n\n\nOr you can edit the columns of a \nDataFrame\n in-place using the \npool!\n function:\n\n\ndf\n \n=\n \nDataFrame\n(\nA\n \n=\n \n[\n1\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n2\n],\n\n               \nB\n \n=\n \n[\nX\n,\n \nX\n,\n \nX\n,\n \nY\n,\n \nY\n,\n \nY\n])\n\n\npool!\n(\ndf\n,\n \n[:\nA\n,\n \n:\nB\n])\n\n\n\n\n\n\nPooling columns is important for working with the \nGLM package\n When fitting regression models, \nPooledDataArray\n columns in the input are translated into 0/1 indicator columns in the \nModelMatrix\n with one column for each of the levels of the \nPooledDataArray\n. This allows one to analyze categorical data efficiently.", 
            "title": "Pooling"
        }, 
        {
            "location": "/man/pooling/#pooling-data-representing-factors", 
            "text": "Often, we have to deal with factors that take on a small number of levels:  dv   =   @ data ([ Group A ,   Group A ,   Group A , \n             Group B ,   Group B ,   Group B ])   The naive encoding used in a  DataArray  represents every entry of this vector as a full string. In contrast, we can represent the data more efficiently by replacing the strings with indices into a small pool of levels. This is what the  PooledDataArray  does:  pdv   =   @ pdata ([ Group A ,   Group A ,   Group A , \n               Group B ,   Group B ,   Group B ])   In addition to representing repeated data efficiently, the  PooledDataArray  allows us to determine the levels of the factor at any time using the  levels  function:  levels ( pdv )   By default, a  PooledDataArray  is able to represent 2 32 differents levels. You can use less memory by calling the  compact  function:  pdv   =   compact ( pdv )   Often, you will have factors encoded inside a DataFrame with  DataArray  columns instead of  PooledDataArray  columns. You can do conversion of a single column using the  pool  function:  pdv   =   pool ( dv )   Or you can edit the columns of a  DataFrame  in-place using the  pool!  function:  df   =   DataFrame ( A   =   [ 1 ,   1 ,   1 ,   2 ,   2 ,   2 ], \n                B   =   [ X ,   X ,   X ,   Y ,   Y ,   Y ])  pool! ( df ,   [: A ,   : B ])   Pooling columns is important for working with the  GLM package  When fitting regression models,  PooledDataArray  columns in the input are translated into 0/1 indicator columns in the  ModelMatrix  with one column for each of the levels of the  PooledDataArray . This allows one to analyze categorical data efficiently.", 
            "title": "Pooling Data (Representing Factors)"
        }, 
        {
            "location": "/lib/maintypes/", 
            "text": "{meta}\nCurrentModule = DataFrames\n\n\n\n\n\n\n\nMain Types\n\n\n{index}\nPages = [\nmaintypes.md\n]\n\n\n\n\n\n...\n\n\n{docs}\nAbstractDataFrame\nDataFrame\nSubDataFrame", 
            "title": "Main types"
        }, 
        {
            "location": "/lib/maintypes/#main-types", 
            "text": "{index}\nPages = [ maintypes.md ]  ...  {docs}\nAbstractDataFrame\nDataFrame\nSubDataFrame", 
            "title": "Main Types"
        }, 
        {
            "location": "/lib/utilities/", 
            "text": "{meta}\nCurrentModule = DataFrames\n\n\n\n\n\n\n\nUtilities\n\n\n{index}\nPages = [\nutilities.md\n]\n\n\n\n\n\n...\n\n\n{docs}\neltypes\nhead\ncomplete_cases\ncomplete_cases!\ndescribe\ndump\nnames!\nnonunique\nrename\nrename!\ntail\nunique\nunique!", 
            "title": "Utilities"
        }, 
        {
            "location": "/lib/utilities/#utilities", 
            "text": "{index}\nPages = [ utilities.md ]  ...  {docs}\neltypes\nhead\ncomplete_cases\ncomplete_cases!\ndescribe\ndump\nnames!\nnonunique\nrename\nrename!\ntail\nunique\nunique!", 
            "title": "Utilities"
        }, 
        {
            "location": "/lib/manipulation/", 
            "text": "{meta}\nCurrentModule = DataFrames\n\n\n\n\n\n\n\nData Manipulation\n\n\n{index}\nPages = [\nmanipulation.md\n]\n\n\n\n\n\n\n\nJoins\n\n\n{docs}\njoin\n\n\n\n\n\n\n\nReshaping\n\n\n{docs}\nmelt \nstack\nunstack\nstackdf\nmeltdf", 
            "title": "Data manipulation"
        }, 
        {
            "location": "/lib/manipulation/#data-manipulation", 
            "text": "{index}\nPages = [ manipulation.md ]", 
            "title": "Data Manipulation"
        }, 
        {
            "location": "/lib/manipulation/#joins", 
            "text": "{docs}\njoin", 
            "title": "Joins"
        }, 
        {
            "location": "/lib/manipulation/#reshaping", 
            "text": "{docs}\nmelt \nstack\nunstack\nstackdf\nmeltdf", 
            "title": "Reshaping"
        }, 
        {
            "location": "/NEWS/", 
            "text": "DataFrames v0.6.11 Release Notes\n\n\n\n\nChanges\n\n\n\n\nNew documentation based on Documenter ([#929])\n\n\nSupport new fit indicator functions for statistical models ([#921]).\n\n\nAdd string literals csv, csv2, wsv, and tsv ([#918])\n\n\nAdd a readtable argument for optional name normalization ([#896])\n\n\n\n\n\n\nDataFrames v0.6.6 Release Notes\n\n\n\n\nDeprecations\n\n\n\n\nDeprecates \narray(df, ...)\n in favor of \nconvert(Array, df, ...)\n ([#806])\n\n\nDeprecates \nDataArray(df, T)\n in favor of \nconvert(DataArray{T}, df)\n ([#806])\n\n\n\n\n\n\nDataFrames v0.6.3 Release Notes\n\n\n\n\nDeprecations\n\n\n\n\nRemoves \nsave\n and \nloaddf\n, since the format was not compatible across Julia and DataFrames versions ([#790]). Use \nwritetable\n or \nJLD\n to save DataFrames\n\n\n\n\n\n\nDataFrames v0.6.1 Release Notes\n\n\n\n\nNew features\n\n\n\n\nwritetable\n supports \nappend\n option ([#755])\n\n\n\n\n\n\nChanges\n\n\n\n\nFaster \nread_rda\n ([#754], [#759])\n\n\n\n\n\n\nDataFrames v0.6.0 Release Notes\n\n\nFocus on performance improvements and rooting out bugs in corner cases.\n\n\n\n\nNew features\n\n\n\n\nConstructor for empty DataFrames allows specifying PDAs ([#725])\n\n\nstack(df)\n and \nmelt(df)\n, which take FloatingPoint vars as measure vars ([#734])\n\n\nNew convenience methods for \nunstack\n ([#734])\n\n\nconvertdataframes\n option added to \nread_rda\n ([#751])\n\n\n\n\n\n\nChanges\n\n\n\n\nvcat(dfs)\n handles container and eltype promotion ([#747])\n\n\njoin\n finally handles DataFrames with no non-key columns ([#749])\n\n\nsorting methods throw an error when args meant for \ncols\n are passed to \nby\n ([#749])\n\n\nrename!\n and \nrename\n throw when column to be renamed does not exist ([#749])\n\n\nnames!\n, \nrename!\n, and \nrename\n for DataFrames now return DataFrames ([#749])\n\n\n\n\n\n\nDeprecations\n\n\n\n\nDeprecates \nby(df, cols, [symbol(s)])\n in favor of \naggregate(df, cols, [function(s)])\n ([#726])\n\n\nRemoves \npivottable\n in favor of other reshaping methods ([#734])\n\n\nDeprecates \nnullable!(..., ::AbstractDataFrame)\n in favor of \nnullable!(::DataFrame, ...)\n ([#752])\n\n\nDeprecates \nkeys(df)\n, \nvalues(df)\n ([#752])\n\n\nRenames \ninsert!(df, df)\n to \nmerge!(df, dfs...)\n ([#752])\n\n\n\n\n\n\nDataFrames v0.5.12 Release Notes\n\n\nTrack changes to JuliaLang/julia\n\n\n\n\nDataFrames v0.5.11 Release Notes\n\n\nTrack changes to JuliaLang/julia\n\n\n\n\nDataFrames v0.5.10 Release Notes\n\n\n\n\nNew features\n\n\n\n\nFormulas handle three-way (and higher) interactions ([#700])\n\n\n\n\n\n\nChanges\n\n\n\n\nNow using ReadTheDocs for documentation\n\n\n\n\n\n\nDataFrames v0.5.9 Release Notes\n\n\nTrack changes to JuliaLang/julia\n\n\n\n\nDataFrames v0.5.8 Release Notes\n\n\n\n\nNew features\n\n\n\n\nExtends \nStatsBase.predict\n to take a DataFrame as a predictor ([#679])\n\n\ncoefnames\n handles random-effect terms ([#662])\n\n\n\n\n\n\nDeprecations\n\n\n\n\nDeprecates \nDataFrame(::Dict, ...)\n in favor of \nconvert\n ([#626])\n\n\n\n\n\n\nDataFrames v0.5.7 Release Notes\n\n\n\n\nNew features\n\n\n\n\ndeleterows!(df::DataFrame, inds)\n ([#635])\n\n\n\n\n\n\nChanges\n\n\n\n\nempty!(::DataFrame)\n and \ninsert!(::DataFrame, ...)\n now operate in place ([#634])\n\n\nAll exported higher-order functions now handle do-block syntax ([#643])\n\n\n\n\n\n\nDataFrames v0.5.6 Release Notes\n\n\nTrack changes to JuliaLang/julia\n\n\n\n\nDataFrames v0.5.5 Release Notes\n\n\n\n\nNew features\n\n\n\n\nSupport fitting arbitrary StatisticalModels ([#571])\n\n\nTest coverage now tracked via Coveralls.io ([#597])\n\n\n\n\n\n\nChanges\n\n\n\n\nshow(::AbstractDataFrame)\n now shows all columns by default\n\n\n\n\n\n\nDeprecations\n\n\n\n\nDeprecates \nDataFrame(::Any...)\n, \nDataFrame(::Associative)\n ([#610])\n\n\n\n\n\n\nDataFrames v0.5.4 Release Notes\n\n\n\n\nNew features\n\n\n\n\npush!\n methods add a row to a DataFrame ([#621])\n\n\nTest coverage now tracked via Coveralls.io ([#597])\n\n\n\n\n\n\nChanges\n\n\n\n\nIO functions ensure column names are valid symbols ([#563])\n\n\nsetindex!\n methods now return the updated DataFrame\n\n\n\n\n\n\nDeprecations\n\n\n\n\nDeprecates \nDataFrame(::Int, ::Int)\n ([#561])\n\n\n\n\n\n\nDataFrames v0.5.3 Release Notes\n\n\nInternal changes to adjust to [JuliaLang/julia#5897]\n\n\n\n\nDataFrames v0.5.2 Release Notes\n\n\nContinues trend of stripping down features and improving core functionality.\n\n\n\n\nNew features\n\n\n\n\nappend!(::AbstractDataFrame, ::AbstractDataFrame)\n ([#506])\n\n\njoin\n supports \n:semi\n-, \n:anti\n- and \n:cross\n-joins ([#524], [#536])\n\n\nImplement \neltypes\n argument in \nreadtable\n ([#497])\n\n\nRead from generic IO objects ([#499])\n\n\n\n\n\n\nChanges\n\n\n\n\nConvert to using only symbols (no more strings) for column names ([#509])\n\n\nRenames \nstack_df\n, \nmelt_df\n, \npivot_table\n to \nstackdf\n, \nmeltdf\n, \npivottable\n ([#538])\n\n\nRenames \nduplicated\n, \ndrop_duplicates!\n to \nnonunique\n, \nunique!\n ([#538])\n\n\nRenames \nload_df\n to \nloaddf\n ([#538])\n\n\nRenames \ntypes\n to \neltypes\n ([#539])\n\n\nRenames \nreadtable\n argument \ncolnames\n to \nnames\n ([#497])\n\n\n\n\n\n\nDeprecations\n\n\n\n\nRemoves expression-based indexing, including \nwith\n, \nwithin!\n, \nbased_on\n, etc. ([#492])\n\n\nRemoves \nDataStream\n ([#492])\n\n\nRemoves \nNamedArray\n ([#492])\n\n\nRemoves column groupings (\nset_groups\n, \nget_groups\n, etc.)  ([#492])\n\n\nRemoves specific colwise and rowwise functions (\nrowsums\n, \ncolnorms\n, etc.) ([#492])\n\n\nRemoves \n@DataFrame\n and \n@transform\n ([#492])\n\n\nDeprecates natural \njoin\ns: the key must be specified now ([#536])\n\n\n\n\n\n\nDataFrames v0.5.1 Release Notes\n\n\nRemoving prototype features until core functionality is farther along.\n\n\n\n\nChanges\n\n\n\n\nWrite \nFormula\ns without quoting, thanks to the \n@~\n macro ([JuliaLang/julia#4882])\n\n\nRenames \nEachCol\n, \nEachRow\n to \neachcol\n, \neachrow\n ([#474])\n\n\neachrow\n returns a \nDataFrameRow\n ([#474])\n\n\nSubDataFrames\n are now immutable ([#474])\n\n\n\n\n\n\nDeprecations\n\n\n\n\nRemoves \nIndexedVector\n ([#483])\n\n\nRemoves \nBlocks.jl\n functionality ([#483])\n\n\nRemoves methods that treat DataFrame like a matrix, e.g \nround\n, \nsin\n ([#484])\n\n\nDeprecates \nsub\n's alias \nsubset\n ([#474])\n\n\n\n\n\n\nDataFrames v0.5.0 Release Notes\n\n\nImproved I/O and more-Julian idioms.\n\n\n\n\nNew features\n\n\n\n\nWrite HTML tables via writemime ([#433])\n\n\nRead whitespace-delimited input ([#443])\n\n\nRead input with C-style escapes ([#454])\n\n\n\n\n\n\nChanges\n\n\n\n\nsort\n interface updated to better match mainline Julia ([#389])\n\n\nnames!\n, \nrename!\n, and \ndelete!\n now return the updated Index, rather than the names in the Index ([#445])\n\n\nRenames \ncoltypes\n, \ncolnames\n, \nclean_colnames!\n to \ntypes\n, \nnames\n, \ncleannames!\n ([#469])\n\n\nVarious improvements to \nprint\n/\nshow\n methods\n\n\n\n\n\n\nDeprecations\n\n\n\n\nDeprecates \nrbind\n, \ncbind\n and \nvecbind\n deprecated in favor of \nhcat\n and \nvcat\n ([#453])\n\n\n\n\n[#389]: https://github.com/JuliaStats/DataFrames.jl/issues/389 [#433]: https://github.com/JuliaStats/DataFrames.jl/issues/433 [#443]: https://github.com/JuliaStats/DataFrames.jl/issues/443 [#445]: https://github.com/JuliaStats/DataFrames.jl/issues/445 [#453]: https://github.com/JuliaStats/DataFrames.jl/issues/453 [#454]: https://github.com/JuliaStats/DataFrames.jl/issues/454 [#469]: https://github.com/JuliaStats/DataFrames.jl/issues/469 [#474]: https://github.com/JuliaStats/DataFrames.jl/issues/474 [#483]: https://github.com/JuliaStats/DataFrames.jl/issues/483 [#484]: https://github.com/JuliaStats/DataFrames.jl/issues/484 [#492]: https://github.com/JuliaStats/DataFrames.jl/issues/492 [#497]: https://github.com/JuliaStats/DataFrames.jl/issues/497 [#499]: https://github.com/JuliaStats/DataFrames.jl/issues/499 [#506]: https://github.com/JuliaStats/DataFrames.jl/issues/506 [#509]: https://github.com/JuliaStats/DataFrames.jl/issues/509 [#524]: https://github.com/JuliaStats/DataFrames.jl/issues/524 [#536]: https://github.com/JuliaStats/DataFrames.jl/issues/536 [#538]: https://github.com/JuliaStats/DataFrames.jl/issues/538 [#539]: https://github.com/JuliaStats/DataFrames.jl/issues/539 [#561]: https://github.com/JuliaStats/DataFrames.jl/issues/561 [#563]: https://github.com/JuliaStats/DataFrames.jl/issues/563 [#571]: https://github.com/JuliaStats/DataFrames.jl/issues/571 [#597]: https://github.com/JuliaStats/DataFrames.jl/issues/597 [#610]: https://github.com/JuliaStats/DataFrames.jl/issues/610 [#621]: https://github.com/JuliaStats/DataFrames.jl/issues/621 [#626]: https://github.com/JuliaStats/DataFrames.jl/issues/626 [#634]: https://github.com/JuliaStats/DataFrames.jl/issues/634 [#635]: https://github.com/JuliaStats/DataFrames.jl/issues/635 [#643]: https://github.com/JuliaStats/DataFrames.jl/issues/643 [#662]: https://github.com/JuliaStats/DataFrames.jl/issues/662 [#679]: https://github.com/JuliaStats/DataFrames.jl/issues/679 [#700]: https://github.com/JuliaStats/DataFrames.jl/issues/700 [#725]: https://github.com/JuliaStats/DataFrames.jl/issues/725 [#726]: https://github.com/JuliaStats/DataFrames.jl/issues/726 [#734]: https://github.com/JuliaStats/DataFrames.jl/issues/734 [#747]: https://github.com/JuliaStats/DataFrames.jl/issues/747 [#749]: https://github.com/JuliaStats/DataFrames.jl/issues/749 [#751]: https://github.com/JuliaStats/DataFrames.jl/issues/751 [#752]: https://github.com/JuliaStats/DataFrames.jl/issues/752 [#754]: https://github.com/JuliaStats/DataFrames.jl/issues/754 [#755]: https://github.com/JuliaStats/DataFrames.jl/issues/755 [#759]: https://github.com/JuliaStats/DataFrames.jl/issues/759 [#790]: https://github.com/JuliaStats/DataFrames.jl/issues/790 [#806]: https://github.com/JuliaStats/DataFrames.jl/issues/806\n\n\n[JuliaLang/julia#4882]: https://github.com/JuliaLang/julia/issues/4882 [JuliaLang/julia#5897]: https://github.com/JuliaLang/julia/issues/5897", 
            "title": "Release notes"
        }, 
        {
            "location": "/NEWS/#dataframes-v0611-release-notes", 
            "text": "", 
            "title": "DataFrames v0.6.11 Release Notes"
        }, 
        {
            "location": "/NEWS/#changes", 
            "text": "New documentation based on Documenter ([#929])  Support new fit indicator functions for statistical models ([#921]).  Add string literals csv, csv2, wsv, and tsv ([#918])  Add a readtable argument for optional name normalization ([#896])", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#dataframes-v066-release-notes", 
            "text": "", 
            "title": "DataFrames v0.6.6 Release Notes"
        }, 
        {
            "location": "/NEWS/#deprecations", 
            "text": "Deprecates  array(df, ...)  in favor of  convert(Array, df, ...)  ([#806])  Deprecates  DataArray(df, T)  in favor of  convert(DataArray{T}, df)  ([#806])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v063-release-notes", 
            "text": "", 
            "title": "DataFrames v0.6.3 Release Notes"
        }, 
        {
            "location": "/NEWS/#deprecations_1", 
            "text": "Removes  save  and  loaddf , since the format was not compatible across Julia and DataFrames versions ([#790]). Use  writetable  or  JLD  to save DataFrames", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v061-release-notes", 
            "text": "", 
            "title": "DataFrames v0.6.1 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features", 
            "text": "writetable  supports  append  option ([#755])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_1", 
            "text": "Faster  read_rda  ([#754], [#759])", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#dataframes-v060-release-notes", 
            "text": "Focus on performance improvements and rooting out bugs in corner cases.", 
            "title": "DataFrames v0.6.0 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_1", 
            "text": "Constructor for empty DataFrames allows specifying PDAs ([#725])  stack(df)  and  melt(df) , which take FloatingPoint vars as measure vars ([#734])  New convenience methods for  unstack  ([#734])  convertdataframes  option added to  read_rda  ([#751])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_2", 
            "text": "vcat(dfs)  handles container and eltype promotion ([#747])  join  finally handles DataFrames with no non-key columns ([#749])  sorting methods throw an error when args meant for  cols  are passed to  by  ([#749])  rename!  and  rename  throw when column to be renamed does not exist ([#749])  names! ,  rename! , and  rename  for DataFrames now return DataFrames ([#749])", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#deprecations_2", 
            "text": "Deprecates  by(df, cols, [symbol(s)])  in favor of  aggregate(df, cols, [function(s)])  ([#726])  Removes  pivottable  in favor of other reshaping methods ([#734])  Deprecates  nullable!(..., ::AbstractDataFrame)  in favor of  nullable!(::DataFrame, ...)  ([#752])  Deprecates  keys(df) ,  values(df)  ([#752])  Renames  insert!(df, df)  to  merge!(df, dfs...)  ([#752])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v0512-release-notes", 
            "text": "Track changes to JuliaLang/julia", 
            "title": "DataFrames v0.5.12 Release Notes"
        }, 
        {
            "location": "/NEWS/#dataframes-v0511-release-notes", 
            "text": "Track changes to JuliaLang/julia", 
            "title": "DataFrames v0.5.11 Release Notes"
        }, 
        {
            "location": "/NEWS/#dataframes-v0510-release-notes", 
            "text": "", 
            "title": "DataFrames v0.5.10 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_2", 
            "text": "Formulas handle three-way (and higher) interactions ([#700])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_3", 
            "text": "Now using ReadTheDocs for documentation", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#dataframes-v059-release-notes", 
            "text": "Track changes to JuliaLang/julia", 
            "title": "DataFrames v0.5.9 Release Notes"
        }, 
        {
            "location": "/NEWS/#dataframes-v058-release-notes", 
            "text": "", 
            "title": "DataFrames v0.5.8 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_3", 
            "text": "Extends  StatsBase.predict  to take a DataFrame as a predictor ([#679])  coefnames  handles random-effect terms ([#662])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#deprecations_3", 
            "text": "Deprecates  DataFrame(::Dict, ...)  in favor of  convert  ([#626])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v057-release-notes", 
            "text": "", 
            "title": "DataFrames v0.5.7 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_4", 
            "text": "deleterows!(df::DataFrame, inds)  ([#635])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_4", 
            "text": "empty!(::DataFrame)  and  insert!(::DataFrame, ...)  now operate in place ([#634])  All exported higher-order functions now handle do-block syntax ([#643])", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#dataframes-v056-release-notes", 
            "text": "Track changes to JuliaLang/julia", 
            "title": "DataFrames v0.5.6 Release Notes"
        }, 
        {
            "location": "/NEWS/#dataframes-v055-release-notes", 
            "text": "", 
            "title": "DataFrames v0.5.5 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_5", 
            "text": "Support fitting arbitrary StatisticalModels ([#571])  Test coverage now tracked via Coveralls.io ([#597])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_5", 
            "text": "show(::AbstractDataFrame)  now shows all columns by default", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#deprecations_4", 
            "text": "Deprecates  DataFrame(::Any...) ,  DataFrame(::Associative)  ([#610])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v054-release-notes", 
            "text": "", 
            "title": "DataFrames v0.5.4 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_6", 
            "text": "push!  methods add a row to a DataFrame ([#621])  Test coverage now tracked via Coveralls.io ([#597])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_6", 
            "text": "IO functions ensure column names are valid symbols ([#563])  setindex!  methods now return the updated DataFrame", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#deprecations_5", 
            "text": "Deprecates  DataFrame(::Int, ::Int)  ([#561])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v053-release-notes", 
            "text": "Internal changes to adjust to [JuliaLang/julia#5897]", 
            "title": "DataFrames v0.5.3 Release Notes"
        }, 
        {
            "location": "/NEWS/#dataframes-v052-release-notes", 
            "text": "Continues trend of stripping down features and improving core functionality.", 
            "title": "DataFrames v0.5.2 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_7", 
            "text": "append!(::AbstractDataFrame, ::AbstractDataFrame)  ([#506])  join  supports  :semi -,  :anti - and  :cross -joins ([#524], [#536])  Implement  eltypes  argument in  readtable  ([#497])  Read from generic IO objects ([#499])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_7", 
            "text": "Convert to using only symbols (no more strings) for column names ([#509])  Renames  stack_df ,  melt_df ,  pivot_table  to  stackdf ,  meltdf ,  pivottable  ([#538])  Renames  duplicated ,  drop_duplicates!  to  nonunique ,  unique!  ([#538])  Renames  load_df  to  loaddf  ([#538])  Renames  types  to  eltypes  ([#539])  Renames  readtable  argument  colnames  to  names  ([#497])", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#deprecations_6", 
            "text": "Removes expression-based indexing, including  with ,  within! ,  based_on , etc. ([#492])  Removes  DataStream  ([#492])  Removes  NamedArray  ([#492])  Removes column groupings ( set_groups ,  get_groups , etc.)  ([#492])  Removes specific colwise and rowwise functions ( rowsums ,  colnorms , etc.) ([#492])  Removes  @DataFrame  and  @transform  ([#492])  Deprecates natural  join s: the key must be specified now ([#536])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v051-release-notes", 
            "text": "Removing prototype features until core functionality is farther along.", 
            "title": "DataFrames v0.5.1 Release Notes"
        }, 
        {
            "location": "/NEWS/#changes_8", 
            "text": "Write  Formula s without quoting, thanks to the  @~  macro ([JuliaLang/julia#4882])  Renames  EachCol ,  EachRow  to  eachcol ,  eachrow  ([#474])  eachrow  returns a  DataFrameRow  ([#474])  SubDataFrames  are now immutable ([#474])", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#deprecations_7", 
            "text": "Removes  IndexedVector  ([#483])  Removes  Blocks.jl  functionality ([#483])  Removes methods that treat DataFrame like a matrix, e.g  round ,  sin  ([#484])  Deprecates  sub 's alias  subset  ([#474])", 
            "title": "Deprecations"
        }, 
        {
            "location": "/NEWS/#dataframes-v050-release-notes", 
            "text": "Improved I/O and more-Julian idioms.", 
            "title": "DataFrames v0.5.0 Release Notes"
        }, 
        {
            "location": "/NEWS/#new-features_8", 
            "text": "Write HTML tables via writemime ([#433])  Read whitespace-delimited input ([#443])  Read input with C-style escapes ([#454])", 
            "title": "New features"
        }, 
        {
            "location": "/NEWS/#changes_9", 
            "text": "sort  interface updated to better match mainline Julia ([#389])  names! ,  rename! , and  delete!  now return the updated Index, rather than the names in the Index ([#445])  Renames  coltypes ,  colnames ,  clean_colnames!  to  types ,  names ,  cleannames!  ([#469])  Various improvements to  print / show  methods", 
            "title": "Changes"
        }, 
        {
            "location": "/NEWS/#deprecations_8", 
            "text": "Deprecates  rbind ,  cbind  and  vecbind  deprecated in favor of  hcat  and  vcat  ([#453])   [#389]: https://github.com/JuliaStats/DataFrames.jl/issues/389 [#433]: https://github.com/JuliaStats/DataFrames.jl/issues/433 [#443]: https://github.com/JuliaStats/DataFrames.jl/issues/443 [#445]: https://github.com/JuliaStats/DataFrames.jl/issues/445 [#453]: https://github.com/JuliaStats/DataFrames.jl/issues/453 [#454]: https://github.com/JuliaStats/DataFrames.jl/issues/454 [#469]: https://github.com/JuliaStats/DataFrames.jl/issues/469 [#474]: https://github.com/JuliaStats/DataFrames.jl/issues/474 [#483]: https://github.com/JuliaStats/DataFrames.jl/issues/483 [#484]: https://github.com/JuliaStats/DataFrames.jl/issues/484 [#492]: https://github.com/JuliaStats/DataFrames.jl/issues/492 [#497]: https://github.com/JuliaStats/DataFrames.jl/issues/497 [#499]: https://github.com/JuliaStats/DataFrames.jl/issues/499 [#506]: https://github.com/JuliaStats/DataFrames.jl/issues/506 [#509]: https://github.com/JuliaStats/DataFrames.jl/issues/509 [#524]: https://github.com/JuliaStats/DataFrames.jl/issues/524 [#536]: https://github.com/JuliaStats/DataFrames.jl/issues/536 [#538]: https://github.com/JuliaStats/DataFrames.jl/issues/538 [#539]: https://github.com/JuliaStats/DataFrames.jl/issues/539 [#561]: https://github.com/JuliaStats/DataFrames.jl/issues/561 [#563]: https://github.com/JuliaStats/DataFrames.jl/issues/563 [#571]: https://github.com/JuliaStats/DataFrames.jl/issues/571 [#597]: https://github.com/JuliaStats/DataFrames.jl/issues/597 [#610]: https://github.com/JuliaStats/DataFrames.jl/issues/610 [#621]: https://github.com/JuliaStats/DataFrames.jl/issues/621 [#626]: https://github.com/JuliaStats/DataFrames.jl/issues/626 [#634]: https://github.com/JuliaStats/DataFrames.jl/issues/634 [#635]: https://github.com/JuliaStats/DataFrames.jl/issues/635 [#643]: https://github.com/JuliaStats/DataFrames.jl/issues/643 [#662]: https://github.com/JuliaStats/DataFrames.jl/issues/662 [#679]: https://github.com/JuliaStats/DataFrames.jl/issues/679 [#700]: https://github.com/JuliaStats/DataFrames.jl/issues/700 [#725]: https://github.com/JuliaStats/DataFrames.jl/issues/725 [#726]: https://github.com/JuliaStats/DataFrames.jl/issues/726 [#734]: https://github.com/JuliaStats/DataFrames.jl/issues/734 [#747]: https://github.com/JuliaStats/DataFrames.jl/issues/747 [#749]: https://github.com/JuliaStats/DataFrames.jl/issues/749 [#751]: https://github.com/JuliaStats/DataFrames.jl/issues/751 [#752]: https://github.com/JuliaStats/DataFrames.jl/issues/752 [#754]: https://github.com/JuliaStats/DataFrames.jl/issues/754 [#755]: https://github.com/JuliaStats/DataFrames.jl/issues/755 [#759]: https://github.com/JuliaStats/DataFrames.jl/issues/759 [#790]: https://github.com/JuliaStats/DataFrames.jl/issues/790 [#806]: https://github.com/JuliaStats/DataFrames.jl/issues/806  [JuliaLang/julia#4882]: https://github.com/JuliaLang/julia/issues/4882 [JuliaLang/julia#5897]: https://github.com/JuliaLang/julia/issues/5897", 
            "title": "Deprecations"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "DataFrames.jl is licensed under the MIT License:\n\n\n\n\nCopyright (c) 2012-2015: Harlan Harris, EPRI (Tom Short's code), Chris DuBois, John Myles White, and other contributors.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }
    ]
}